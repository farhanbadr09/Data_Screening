{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Pipeline ETL for pushing it into an AWS RDS database \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries and credentials to access AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(\"/credentials.json\")\n",
    "credentials = json.load(f)\n",
    "\n",
    "file_path = credentials['file_path']\n",
    "aws_key = credentials['aws_access_key']\n",
    "aws_secret_key = credentials['aws_secret_key']\n",
    "rds_host = credentials['rds_host']\n",
    "rds_user = credentials['rds_user']\n",
    "rds_password = credentials['rds_password']\n",
    "rds_database = credentials['rds_database']\n",
    "rds_charset = credentials['rds_charset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "import time\n",
    "\n",
    "ApiClient().configuration.api_key['api_key'] = aws_key\n",
    "\n",
    "# Import the usual Python libraries\n",
    "\n",
    "from tqdm.notebook import tqdm, trange  # to be used to track progress in loop iterations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Zip file libraries\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import urllib.request as urllib2\n",
    "\n",
    "# Import the AWS libraries\n",
    "\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Declare the local file path to be used for saving CSV outputs.\n",
    "\n",
    "global my_path\n",
    "my_path = file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Stocks data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after downloading the data, roll them up into a single dataframe for transformation into the shape we need for the DB table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble stocks history into one dataframe.\n",
    "\n",
    "def assemble_bulk_history():\n",
    "    \n",
    "    global df_history\n",
    "\n",
    "    history = np.empty([0,29])\n",
    "\n",
    "    for X in tqdm(range(1, 2)):\n",
    "\n",
    "        file_path = my_path + \"/\" + \"MSFT\" + str(X) + \".csv\"\n",
    "        data = pd.read_csv (file_path, low_memory = False)\n",
    "        data = np.array(data.values)\n",
    "        history = np.concatenate((volume, data), axis=0)\n",
    "\n",
    "    # Convert data array to dataframe and do some cleanup\n",
    "\n",
    "    df_history = pd.DataFrame(data = history, columns = ['date', 'open', 'high', 'low', 'close', 'adj_close','volume'])\n",
    "\n",
    "    # Make sure Date column is in DateTime format, \n",
    "\n",
    "    df_history['date'] = pd.to_datetime(df_history['date'])\n",
    "    df_history = df_history.sort_values(by=['volume', 'date'])\n",
    "\n",
    "    print(\"history files assembled.\")    \n",
    "    print(\"The shape of the history dataframe is \", df_history.shape)\n",
    "    \n",
    "    return df_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# For backup or archive purposes, save the final dataframe to CSV and/or parquet files and push them to AWS S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the low level functional AWS client\n",
    "\n",
    "def push_data_to_S3(df_history_complete_load):\n",
    "    \n",
    "    client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id = aws_key,\n",
    "        aws_secret_access_key = aws_secret_key,\n",
    "        region_name = 'us-east-1'\n",
    "    )\n",
    "\n",
    "    # Export the history dataframe to a zipped CSV file then push to AWS S3.\n",
    "    compression_opts = dict(method='zip', archive_name='df_full_history_complete_load.csv') \n",
    "    df_volume_history_complete_load.to_csv(path_or_buf = my_path + \"/MSFT.csv\", index=False, compression=compression_opts)\n",
    "    client.upload_file(my_path + \"/MSFT.csv\")\n",
    "\n",
    "\n",
    "    # Write parquet file to local drive, then push to AWS S3.\n",
    "    local_file = my_path + \"/df_full_history_complete_load.parquet\"\n",
    "    stock_table = pa.Table.from_pandas(df_volume)\n",
    "    pq.write_table(stock_table, local_file)\n",
    "    client.upload_file(local_file,\"data-historical/parquet_files/df_full_history_complete_load.parquet\")\n",
    "\n",
    "    print(\"Data saved to S3 in zipped CSV and parquet.\")\n",
    "    S3_push_status = \"Done.\"\n",
    "    \n",
    "    return S3_push_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# file_path = my_path + \"/\" + \"df_history_complete.zip\"\n",
    "# df_history_complete_load = pd.read_csv (file_path, low_memory=False)\n",
    "# df_history_complete_load.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, create the MySQL history table in RDS and push the history data into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SQL libraries\n",
    "\n",
    "def create_and_fill_RDS_table(df_history_complete_load):\n",
    "\n",
    "    import mysql.connector \n",
    "    from mysql.connector import errorcode\n",
    "\n",
    "    from sqlalchemy import create_engine\n",
    "\n",
    "    # Establish the MySQL connection\n",
    "\n",
    "    connection = mysql.connector.connect(host=rds_host,\n",
    "                                 user=rds_user, \n",
    "                                 password=rds_password, \n",
    "                                 database=rds_database,\n",
    "                                 charset=rds_charset)\n",
    "\n",
    "    mycursor = connection.cursor()\n",
    "\n",
    "    # Create the data table in MySQL with MySQL Connector library\n",
    "\n",
    "    create_data_history_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS `data_historical` (\n",
    "    `key_id` varchar(30) NOT NULL,\n",
    "    `date` datetime NOT NULL,\n",
    "    `open` float NULL,\n",
    "    `high` float NULL,\n",
    "    `low` float NULL,\n",
    "    `close` float NULL,\n",
    "    `adj_close` float NULL,\n",
    "    `volume` float NULL,\n",
    "    `PRIMARY KEY (key_id)\n",
    "    ) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n",
    "    \"\"\"\n",
    "\n",
    "    mycursor.execute(create_data_history_table)\n",
    "\n",
    "    connection.commit()\n",
    "\n",
    "    print(\"The data_historical table is created in RDS.\")\n",
    "\n",
    "\n",
    "    # Push the final dataframe into SQL DB on AWS RDS.\n",
    "\n",
    "    df = df_history_complete_load.copy()\n",
    "\n",
    "    # Set SQLAlchemy database credentials.\n",
    "    creds = {'usr': rds_user,\n",
    "             'pwd': rds_password,\n",
    "             'hst': rds_host,\n",
    "             'prt': 3306,\n",
    "             'dbn': rds_database}\n",
    "\n",
    "    # MySQL conection string.\n",
    "    connstr = 'mysql+mysqlconnector://{usr}:{pwd}@{hst}:{prt}/{dbn}'\n",
    "\n",
    "    # Create sqlalchemy engine for MySQL connection.\n",
    "    engine = create_engine(connstr.format(**creds))\n",
    "\n",
    "    # Write DataFrame to MySQL using the engine (connection) created above.\n",
    "    chunk = int(len(df) / 1000)\n",
    "    df.to_sql(name='data_historical', \n",
    "                                          con=engine, \n",
    "                                          if_exists='replace', \n",
    "                                          chunksize=chunk,\n",
    "                                          index=False)\n",
    "\n",
    "    print(\"The history data is loaded and the indexes are set.\")\n",
    "    rds_table_status = \"Done.\"\n",
    "    \n",
    "    return rds_table_status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the ETL process.\n",
    "\n",
    "assemble_bulk_history\n",
    "push_data_to_S3(df_history_complete_load)\n",
    "create_and_fill_RDS_table(df_history_complete_load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
